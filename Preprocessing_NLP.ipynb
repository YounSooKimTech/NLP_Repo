{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOn2YS+e0kgZvJH20dQG43e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YounSooKimTech/NLP_Repo/blob/main/Preprocessing_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# preprocessing\n",
        "\n",
        "- tokenization\n",
        "- stopwords/punc removal\n",
        "- normalization via stemming(어간추출) and lemmatization(표제어 추출)\n",
        "- POS tagging (품사태깅, 문맥파악)"
      ],
      "metadata": {
        "id": "TOrIUJnoMX_a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WE2CJe4ZMVnx",
        "outputId": "b2c54de4-d769-4a7b-c07a-0f28e9cc381a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.64.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/webtext.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Error loading stopwrods: Package 'stopwrods' not found in\n",
            "[nltk_data]     index\n",
            "[nltk_data] Error loading average_perceptron_tagger: Package\n",
            "[nltk_data]     'average_perceptron_tagger' not found in index\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "! pip install nltk\n",
        "\n",
        "import nltk\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"webtext\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"stopwrods\")\n",
        "nltk.download(\"average_perceptron_tagger\")\n",
        "nltk.download(\"omw-1.4\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# nltk.tokenize"
      ],
      "metadata": {
        "id": "C9x0hH2_Tjjz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "para = \"\"\"Hello everyone.\n",
        " It's good to see you. \n",
        " Let's start out text mining class!\n",
        " \"\"\"\n",
        "\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "sents = sent_tokenize(para)\n",
        "print([sent for sent in sents], \"\\n\")\n",
        "\n",
        "words = word_tokenize(para)\n",
        "\n",
        "print([word+\"\\n\" for word in words])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KglyzHQcMfGE",
        "outputId": "80a67e43-d2ac-47f3-89fe-d64767450aa6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello everyone.', \"It's good to see you.\", \"Let's start out text mining class!\"] \n",
            "\n",
            "['Hello\\n', 'everyone\\n', '.\\n', 'It\\n', \"'s\\n\", 'good\\n', 'to\\n', 'see\\n', 'you\\n', '.\\n', 'Let\\n', \"'s\\n\", 'start\\n', 'out\\n', 'text\\n', 'mining\\n', 'class\\n', '!\\n']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "tokenizer = RegexpTokenizer(\"[\\w]{3,}\")\n",
        "\n",
        "tokenizer.tokenize(\"sorry, I can;t go there\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDf3RaVFOwly",
        "outputId": "9ece0c1c-c3d2-4861-f6ee-51a3b20019fa"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['sorry', 'can', 'there']"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Regular expression을 사용하는 \n",
        "\n",
        "import re\n",
        "\n",
        "print(re.findall(\"[abc]\", \"How are you boys?\"))\n",
        "\n",
        "print(re.findall(\"[0-9]\", \"3a7b5c9d\"))\n",
        "\n",
        "# pattern \\w = [a-zA-Z0-9] 공백이 포함되지 않는다\n",
        "# \\w + 면 공백 쉼표로 구분되는 단어 찾을수 있음\n",
        "\n",
        "print(re.findall(\"[\\w]+\", \"How are you, boys?\"))\n",
        "\n",
        "print(re.findall(\"[o]{2,4}\", \"Oh, how are yoou? booooy\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFdv8DmNN9rG",
        "outputId": "d2d3826e-1cb3-4fbe-dbf8-f419f1513c6b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['a', 'b']\n",
            "['3', '7', '5', '9']\n",
            "['How', 'are', 'you', 'boys']\n",
            "['oo', 'oooo']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# noise and stopwords\n",
        "# 대개 영어는 길이가 3미만인 단어는 삭제하는것이 일반적\n",
        "import nltk\n",
        "\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "english_stop = stopwords.words(\"english\")\n",
        "english_stop[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MeyZOhBlRMh9",
        "outputId": "583827de-2665-4721-f19c-283f2c03d490"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# nltk.corpus stopwords"
      ],
      "metadata": {
        "id": "gQb3TmMeTmds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize --> remove stopwords and len(word)>=3\n",
        "\n",
        "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "text1 = \"sorry, I couldn't go to the movie yesterday\"\n",
        "\n",
        "eng_stopwords = stopwords.words(\"english\")\n",
        "eng_stopwords = set(eng_stopwords)\n",
        "\n",
        "tokenizer = RegexpTokenizer(\"[\\w]+\")\n",
        "\n",
        "\n",
        "words = tokenizer.tokenize(text1)\n",
        "print([word.lower() for word in words if word not in eng_stopwords and len(word)>3])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dR3TZKNR-Ym",
        "outputId": "7c38e4cc-753a-448b-9620-74217237cd69"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['sorry', 'movie', 'yesterday']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_stopwords = [\"I\", \"Go\", \"to\"]\n",
        "\n",
        "print([word for word in words if word not in my_stopwords and len(word)>3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nyZgFt4rSbdt",
        "outputId": "a1431df2-3e49-46fe-ca3b-71e601d4ad9c"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['sorry', 'couldn', 'movie', 'yesterday']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# nltk.stem"
      ],
      "metadata": {
        "id": "Olk64JakTfgi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Porter Stemmer\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "stemmer.stem(\"cooking cookery cookies\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "VnrDlVPeTSim",
        "outputId": "346e12cd-3d56-4aa8-dc8c-07404d3bbbea"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cooking cookery cooki'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text1 = \"Hello, everyone. It's good to see you. Let's start our text mining class!\"\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "tokens = word_tokenize(text1.lower())\n",
        "print(tokens)\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from nltk.stem import PorterStemmer, LancasterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "print([stemmer.stem(token) for token in tokens])\n",
        "\n",
        "'''\n",
        "stemmerL = LancasterStemmer()\n",
        "print([stemmerL.stem(token) for token in tokens])\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 93
        },
        "id": "dCqkVZ_SUNvQ",
        "outputId": "57f92c69-c5a0-49e1-9c2c-8c4312dd831e"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hello', ',', 'everyone', '.', 'it', \"'s\", 'good', 'to', 'see', 'you', '.', 'let', \"'s\", 'start', 'our', 'text', 'mining', 'class', '!']\n",
            "['hello', ',', 'everyon', '.', 'it', \"'s\", 'good', 'to', 'see', 'you', '.', 'let', \"'s\", 'start', 'our', 'text', 'mine', 'class', '!']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nstemmerL = LancasterStemmer()\\nprint([stemmerL.stem(token) for token in tokens])\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Word Net Lemma\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "print(lemmatizer.lemmatize(\"cooking\"))\n",
        "print(lemmatizer.lemmatize(\"cooking\", pos=\"v\"))\n",
        "\n",
        "\n",
        "print(lemmatizer.lemmatize(\"cookbooks\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8rzoPW4UlTe",
        "outputId": "39257fc3-6fd5-46d8-f0ae-a9c03826fb9e"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cooking\n",
            "cook\n",
            "cookbook\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# nltk.pos_tag\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "#nltk.download(\"averaged_perceptron_tagger\")\n",
        "\n",
        "text = '''\n",
        "Hello, everyone.\n",
        "It's gooe to see you\n",
        "Let's start out text mining class :>\n",
        "'''\n",
        "\n",
        "tokens = word_tokenize(text.lower())\n",
        "\n",
        "print(nltk.pos_tag(tokens))\n",
        "print(\"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LETCzOUyWgzz",
        "outputId": "815bab6b-83bd-47d1-a2ab-8823f673a7ba"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('hello', 'NN'), (',', ','), ('everyone', 'NN'), ('.', '.'), ('it', 'PRP'), (\"'s\", 'VBZ'), ('gooe', 'JJ'), ('to', 'TO'), ('see', 'VB'), ('you', 'PRP'), ('let', 'VB'), (\"'s\", 'POS'), ('start', 'VB'), ('out', 'RP'), ('text', 'NN'), ('mining', 'NN'), ('class', 'NN'), (':', ':'), ('>', 'NN')]\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for word, tag in nltk.pos_tag(tokens):\n",
        "  if tag in [\"NN\", \"VB\"]:\n",
        "    print(word, tag)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVGP_t6bYGWU",
        "outputId": "403a9d86-6e69-4b87-d034-b5a2805ccc59"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello NN\n",
            "everyone NN\n",
            "see VB\n",
            "let VB\n",
            "start VB\n",
            "text NN\n",
            "mining NN\n",
            "class NN\n",
            "> NN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install KoNLPy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rprBlzI7Y_PH",
        "outputId": "99116c68-6108-42c6-8bf2-4ae18c451f66"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting KoNLPy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4 MB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from KoNLPy) (1.21.6)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from KoNLPy) (4.9.1)\n",
            "Collecting JPype1>=0.7.0\n",
            "  Downloading JPype1-1.4.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (465 kB)\n",
            "\u001b[K     |████████████████████████████████| 465 kB 65.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->KoNLPy) (21.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->KoNLPy) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->JPype1>=0.7.0->KoNLPy) (3.0.9)\n",
            "Installing collected packages: JPype1, KoNLPy\n",
            "Successfully installed JPype1-1.4.1 KoNLPy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8m9yWPj-ZA29"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}